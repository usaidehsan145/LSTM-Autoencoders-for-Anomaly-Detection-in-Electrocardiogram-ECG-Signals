# -*- coding: utf-8 -*-
"""LSTM_autoencoders_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YOARknQ0av3l6kLOrSuFVS_q6prB44Oi
"""

!gdown --id 16MIleqoIr1vYxlGk4GKnGmrsCPuWkkpT
!unzip -qq ECG5000.zip

!pip install -q liac-arff

# Commented out IPython magic to ensure Python compatibility.
from sklearn.metrics import confusion_matrix, classification_report
from glob import glob
import time
import copy
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
import shutil
import torch
from torch import nn, optim
import torch.nn.functional as F
from torchvision import datasets, transforms, models
import arff
import pandas as pd
import numpy as np

# %matplotlib inline
# %config InlineBackend.figure_format= 'retina'
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

with open('ECG5000_TRAIN.arff','r') as f:
  train = arff.load(f)

train = pd.DataFrame(train['data'], columns=[attr[0] for attr in train['attributes']])
train.shape

with open('ECG5000_TEST.arff','r') as f:
  test = arff.load(f)

test = pd.DataFrame(test['data'], columns=[attr[0] for attr in test['attributes']])
test.shape

df = train.append(test)
df = df.sample(frac=1.0)
df.head()

"""CLASS LABELS"""

CLASS_NORMAL = 1
class_names = ['Normal','R on T', 'PVC', 'SP', 'UB']

df.target.value_counts()

# ax = sns.countplot(df.target)
# ax.set_xticklabels(class_names)

# Assuming `class_names` is a list of labels
class_names = ['Normal','R on T', 'SP', 'PVC', 'UB']

ax = sns.countplot(x='target', data=df)
ax.set_xticklabels(class_names)

plt.show()

normal_df = df[df.target == str(CLASS_NORMAL)].drop(labels = 'target', axis =1)

normal_df.shape

anomaly_df = df[df.target != str(CLASS_NORMAL)].drop(labels = 'target', axis =1)

anomaly_df.shape

train_df, val_df = \
train_test_split(normal_df, test_size= 0.15, random_state=RANDOM_SEED)

val_df, test_df = \
train_test_split(val_df, test_size= 0.15, random_state=RANDOM_SEED)

train_sequences = train_df.astype(np.float32).to_numpy().tolist()
val_sequences = val_df.astype(np.float32).to_numpy().tolist()
test_sequences = test_df.astype(np.float32).to_numpy().tolist()
anomaly_sequences = anomaly_df.astype(np.float32).to_numpy().tolist()

train_sequences

def create_dataset(sequences):
  dataset = [torch.tensor(s).unsqueeze(1) for s in sequences]
  n_seq, seq_len, n_features = torch.stack(dataset).shape
  return dataset, seq_len, n_features

train_dataset, seq_len, n_features = create_dataset(train_sequences)
val_dataset, _,_ = create_dataset(val_sequences)
test_normal_dataset, _, _ = create_dataset(test_sequences)
test_anomaly_dataset, _, _ = create_dataset(anomaly_sequences)

class Encoder(nn.Module):
  def __init__(self, seq_len, n_features, embedding_dim=64):
    super(Encoder, self).__init__()
    self.seq_len, self.n_features = seq_len, n_features
    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim
    self.rnn1 = nn.LSTM(
      input_size=n_features,
      hidden_size=self.hidden_dim,
      num_layers=1,
      batch_first=True
    )
    self.rnn2 = nn.LSTM(
      input_size=self.hidden_dim,
      hidden_size=embedding_dim,
      num_layers=1,
      batch_first=True
    )
  def forward(self, x):
    x = x.reshape((1, self.seq_len, self.n_features))
    x, (_, _) = self.rnn1(x)
    x, (hidden_n, _) = self.rnn2(x)
    return hidden_n.reshape((self.n_features, self.embedding_dim))

class Decoder(nn.Module):
  def __init__(self, seq_len, input_dim=64, n_features=1):
    super(Decoder, self).__init__()
    self.seq_len, self.input_dim = seq_len, input_dim
    self.hidden_dim, self.n_features = 2 * input_dim, n_features
    self.rnn1 = nn.LSTM(
      input_size=input_dim,
      hidden_size=input_dim,
      num_layers=1,
      batch_first=True
    )
    self.rnn2 = nn.LSTM(
      input_size=input_dim,
      hidden_size=self.hidden_dim,
      num_layers=1,
      batch_first=True
    )
    self.output_layer = nn.Linear(self.hidden_dim, n_features)
  def forward(self, x):
    x = x.repeat(self.seq_len, self.n_features)
    x = x.reshape((self.n_features, self.seq_len, self.input_dim))
    x, (hidden_n, cell_n) = self.rnn1(x)
    x, (hidden_n, cell_n) = self.rnn2(x)
    x = x.reshape((self.seq_len, self.hidden_dim))
    return self.output_layer(x)

class RecurrentAutoencoder(nn.Module):
  def __init__(self, seq_len, n_features, embedding_dim=64):
    super(RecurrentAutoencoder, self).__init__()
    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)
    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)
  def forward(self, x):
    x = self.encoder(x)
    x = self.decoder(x)
    return x

model = RecurrentAutoencoder(seq_len, n_features, 128)
model = model.to(device)

def train_model(model, train_dataset, val_dataset, n_epochs):
  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
  criterion = nn.L1Loss(reduction='sum').to(device)
  history = dict(train=[], val=[])
  best_model_wts = copy.deepcopy(model.state_dict())
  best_loss = 10000.0
  for epoch in range(1, n_epochs + 1):
    model = model.train()
    train_losses = []
    for seq_true in train_dataset:
      optimizer.zero_grad()
      seq_true = seq_true.to(device)
      seq_pred = model(seq_true)
      loss = criterion(seq_pred, seq_true)
      loss.backward()
      optimizer.step()
      train_losses.append(loss.item())
    val_losses = []
    model = model.eval()
    with torch.no_grad():
      for seq_true in val_dataset:
        seq_true = seq_true.to(device)
        seq_pred = model(seq_true)
        loss = criterion(seq_pred, seq_true)
        val_losses.append(loss.item())
    train_loss = np.mean(train_losses)
    val_loss = np.mean(val_losses)
    history['train'].append(train_loss)
    history['val'].append(val_loss)
    if val_loss < best_loss:
      best_loss = val_loss
      best_model_wts = copy.deepcopy(model.state_dict())
    print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')
  model.load_state_dict(best_model_wts)
  return model.eval(), history

model, history = train_model(model,train_dataset,val_dataset,n_epochs=150)

ax = plt.figure().gca()
ax.plot(history['train'])
ax.plot(history['val'])
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'])
plt.title('loss over training epoch')
plt.show()

def predict(model, dataset):
  predictions, losses = [], []
  criterion = nn.L1Loss(reduction='sum').to(device)
  with torch.no_grad():
    model = model.eval()
    for seq_true in dataset:
      seq_true = seq_true.to(device)
      seq_pred = model(seq_true)
      loss = criterion(seq_pred, seq_true)
      predictions.append(seq_pred.cpu().numpy().flatten())
      losses.append(loss.item())
  return predictions, losses

_, losses = predict(model, train_dataset)

sns.displot(losses, bins=50,kde=True)

THRESHOLD = 22

predictions, pred_losses = predict(model, test_normal_dataset)

sns.displot(pred_losses, bins=50,kde=True)

correct = sum(l<= THRESHOLD for l in pred_losses)
print(f'correct normal predictions: {correct} / {len(test_normal_dataset)}')

anomaly_dataset = test_anomaly_dataset[:len(test_normal_dataset)]

predictions, pred_losses = predict(model, anomaly_dataset)

sns.displot(pred_losses, bins=50,kde=True)

correct = sum(l<= THRESHOLD for l in pred_losses)
print(f'correct anomaly predictions: {correct} / {len(anomaly_dataset)}')